{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mcdBmTvM_stF",
   "metadata": {
    "id": "mcdBmTvM_stF",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yVojFh-L_67s",
   "metadata": {
    "id": "yVojFh-L_67s"
   },
   "outputs": [],
   "source": [
    "MASTER_CONFIG = {\n",
    "    # Adding parameters later\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gmEUKSc6_7JL",
   "metadata": {
    "id": "gmEUKSc6_7JL"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "file_name = \"tinyshakespeare.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gHKRIpn7AH-L",
   "metadata": {
    "id": "gHKRIpn7AH-L"
   },
   "outputs": [],
   "source": [
    "lines = open(\"tinyshakespeare.txt\", 'r').read()\n",
    "\n",
    "vocab = sorted(list(set(lines)))\n",
    "\n",
    "print('Printing the first 10 characters of the vocab list:', vocab[:10])\n",
    "\n",
    "print('Total number of characters in our dataset (Vocabulary Size):', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zUniCmFcAIJt",
   "metadata": {
    "id": "zUniCmFcAIJt"
   },
   "outputs": [],
   "source": [
    "itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-NerImtWAIUl",
   "metadata": {
    "id": "-NerImtWAIUl"
   },
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "decode(encode(\"morning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-KHfKkFiAbyc",
   "metadata": {
    "id": "-KHfKkFiAbyc"
   },
   "outputs": [],
   "source": [
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)\n",
    "\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KnqC_65JAcD7",
   "metadata": {
    "id": "KnqC_65JAcD7"
   },
   "outputs": [],
   "source": [
    "def get_batches(data, split, batch_size, context_window, config=MASTER_CONFIG):\n",
    "    train = data[:int(.8 * len(data))]\n",
    "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
    "    test = data[int(.9 * len(data)):]\n",
    "\n",
    "    batch_data = train\n",
    "    if split == 'val':\n",
    "        batch_data = val\n",
    "    if split == 'test':\n",
    "        batch_data = test\n",
    "\n",
    "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
    "\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54YnNtnNAnCj",
   "metadata": {
    "id": "54YnNtnNAnCj"
   },
   "outputs": [],
   "source": [
    "MASTER_CONFIG.update({\n",
    "    'batch_size': 8,          # Number of batches to be processed at each random split\n",
    "    'context_window': 16      # Number of characters in each input (x) and target (y) sequence of each batch\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jeIpHGaFAcKZ",
   "metadata": {
    "id": "jeIpHGaFAcKZ"
   },
   "outputs": [],
   "source": [
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "decoded_samples = [(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))]\n",
    "\n",
    "print(decoded_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JVU39CSkAy8h",
   "metadata": {
    "id": "JVU39CSkAy8h"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()  # Don't compute gradients for this function\n",
    "def evaluate_loss(model, config=MASTER_CONFIG):\n",
    "    out = {}\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "\n",
    "        for _ in range(10):\n",
    "            xb, yb = get_batches(dataset, split, config['batch_size'], config['context_window'])\n",
    "\n",
    "            _, loss = model(xb, yb)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        out[split] = np.mean(losses)\n",
    "\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q0yrg20qAzK6",
   "metadata": {
    "id": "Q0yrg20qAzK6"
   },
   "outputs": [],
   "source": [
    "class SimpleBrokenModel(nn.Module):\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            nn.ReLU(),  # Currently using ReLU, will be replaced with SwiGLU as in LLaMA\n",
    "            nn.Linear(config['d_model'], config['vocab_size']),\n",
    "        )\n",
    "\n",
    "        print(\"Model parameters:\", sum([m.numel() for m in self.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ii4i6CRcAzdL",
   "metadata": {
    "id": "ii4i6CRcAzdL"
   },
   "outputs": [],
   "source": [
    "class SimpleBrokenModel(nn.Module):\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "\n",
    "        ...\n",
    "\n",
    "        def forward(self, idx, targets=None):\n",
    "            x = self.embedding(idx)\n",
    "\n",
    "            a = self.linear(x)\n",
    "\n",
    "            logits = F.softmax(a, dim=-1)\n",
    "\n",
    "            if targets is not None:\n",
    "                loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "                return logits, loss\n",
    "\n",
    "            else:\n",
    "                return logits\n",
    "\n",
    "        print(\"Model parameters:\", sum([m.numel() for m in self.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2TEcr7S6AzoY",
   "metadata": {
    "id": "2TEcr7S6AzoY"
   },
   "outputs": [],
   "source": [
    "MASTER_CONFIG.update({\n",
    "    'd_model': 128,\n",
    "})\n",
    "\n",
    "model = SimpleBrokenModel(MASTER_CONFIG)\n",
    "\n",
    "print(\"Total number of parameters in the Simple Neural Network Model:\", sum([m.numel() for m in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ipj_PZbRAzwR",
   "metadata": {
    "id": "ipj_PZbRAzwR"
   },
   "outputs": [],
   "source": [
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "logits, loss = model(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lZWLkWnMBFV5",
   "metadata": {
    "id": "lZWLkWnMBFV5"
   },
   "outputs": [],
   "source": [
    "MASTER_CONFIG.update({\n",
    "    'epochs': 1000,          # Number of training epochs\n",
    "    'log_interval': 10,      # Log information every 10 batches during training\n",
    "    'batch_size': 32,        # Increase batch size to 32\n",
    "})\n",
    "\n",
    "model = SimpleBrokenModel(MASTER_CONFIG)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),      # Pass the model parameters to the optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A_0N_SDrBFYq",
   "metadata": {
    "id": "A_0N_SDrBFYq"
   },
   "outputs": [],
   "source": [
    "Letâ€™s execute the training process and capture the loss from our base model, including the total number of parameters. **Additionally, each line is commented for clarity**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bGU-dKCNBFbI",
   "metadata": {
    "id": "bGU-dKCNBFbI"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler=None, config=MASTER_CONFIG, print_logs=False):\n",
    "    losses = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])\n",
    "\n",
    "        logits, loss = model(xs, targets=ys)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        if epoch % config['log_interval'] == 0:\n",
    "            batch_time = time.time() - start_time\n",
    "\n",
    "            x = evaluate_loss(model)\n",
    "\n",
    "            losses += [x]\n",
    "\n",
    "            if print_logs:\n",
    "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config['epochs'] - epoch)/config['log_interval'] :.3f}\")\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            if scheduler:\n",
    "                print(\"lr: \", scheduler.get_lr())\n",
    "\n",
    "    print(\"Validation loss: \", losses[-1]['val'])\n",
    "\n",
    "    return pd.DataFrame(losses).plot()\n",
    "\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6LgoxwwBFh7",
   "metadata": {
    "id": "f6LgoxwwBFh7"
   },
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "\n",
    "       ...\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embedding(idx)\n",
    "\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        if targets is not None:\n",
    "\n",
    "            ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jEbOYSn8BRpJ",
   "metadata": {
    "id": "jEbOYSn8BRpJ"
   },
   "source": [
    "Letâ€™s recreate the updated SimpleModel and train it for 1000 epochs to observe any changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I9INhopSBRsl",
   "metadata": {
    "id": "I9INhopSBRsl"
   },
   "outputs": [],
   "source": [
    "# Create the updated SimpleModel\n",
    "model = SimpleModel(MASTER_CONFIG)\n",
    "\n",
    "# Obtain batches for training\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Calculate logits and loss using the model\n",
    "logits, loss = model(xs, ys)\n",
    "\n",
    "# Define the Adam optimizer for model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model for 100 epochs\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZadP0622BRyS",
   "metadata": {
    "id": "ZadP0622BRyS"
   },
   "outputs": [],
   "source": [
    "# Generate function for text generation using the trained model\n",
    "def generate(model, config=MASTER_CONFIG, max_new_tokens=30):\n",
    "    idx = torch.zeros(5, 1).long()\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Call the model\n",
    "        logits = model(idx[:, -config['context_window']:])\n",
    "        last_time_step_logits = logits[\n",
    "            :, -1, :\n",
    "        ]  # all the batches (1), last time step, all the logits\n",
    "        p = F.softmax(last_time_step_logits, dim=-1)  # softmax to get probabilities\n",
    "        idx_next = torch.multinomial(\n",
    "            p, num_samples=1\n",
    "        )  # sample from the distribution to get the next token\n",
    "        idx = torch.cat([idx, idx_next], dim=-1)  # append to the sequence\n",
    "    return [decode(x) for x in idx.tolist()]\n",
    "\n",
    "# Generate text using the trained model\n",
    "generate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zALPXzoWBgWH",
   "metadata": {
    "id": "zALPXzoWBgWH"
   },
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, layer_shape, eps=1e-8, bias=False):\n",
    "        super(RMSNorm, self).__init__()\n",
    "\n",
    "        # Registering a learnable parameter 'scale' as a parameter of the module\n",
    "        self.register_parameter(\"scale\", nn.Parameter(torch.ones(layer_shape)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Assumes shape is (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Calculating the Frobenius norm, RMS = 1/sqrt(N) * Frobenius norm\n",
    "        ff_rms = torch.linalg.norm(x, dim=(1,2)) * x[0].numel() ** -.5\n",
    "\n",
    "        # Normalizing the input tensor 'x' with respect to RMS\n",
    "        raw = x / ff_rms.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # Scaling the normalized tensor using the learnable parameter 'scale'\n",
    "        return self.scale[:x.shape[1], :].unsqueeze(0) * raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z8jn0vKUBgcM",
   "metadata": {
    "id": "z8jn0vKUBgcM"
   },
   "outputs": [],
   "source": [
    "# Define the SimpleModel_RMS with RMSNorm\n",
    "class SimpleModel_RMS(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding layer to convert character indices to vectors\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "\n",
    "        # RMSNorm layer for pre-normalization\n",
    "        self.rms = RMSNorm((config['context_window'], config['d_model']))\n",
    "\n",
    "        # Linear layers for modeling relationships between features\n",
    "        self.linear = nn.Sequential(\n",
    "            # Rest of the code\n",
    "            ...\n",
    "        )\n",
    "\n",
    "        # Print the total number of model parameters\n",
    "        print(\"Model parameters:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Embedding layer converts character indices to vectors\n",
    "        x = self.embedding(idx)\n",
    "\n",
    "        # RMSNorm pre-normalization\n",
    "        x = self.rms(x)\n",
    "\n",
    "        # Linear layers for modeling relationships between features\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        if targets is not None:\n",
    "\n",
    "            # Rest of the code\n",
    "            ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9KNj5p-kBgg6",
   "metadata": {
    "id": "9KNj5p-kBgg6"
   },
   "outputs": [],
   "source": [
    "# Create an instance of SimpleModel_RMS\n",
    "model = SimpleModel_RMS(MASTER_CONFIG)\n",
    "\n",
    "# Obtain batches for training\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Calculate logits and loss using the model\n",
    "logits, loss = model(xs, ys)\n",
    "\n",
    "# Define the Adam optimizer for model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NMH7_RuOBgrU",
   "metadata": {
    "id": "NMH7_RuOBgrU"
   },
   "outputs": [],
   "source": [
    "def get_rotary_matrix(context_window, embedding_dim):\n",
    "    # Initialize a tensor for the rotary matrix with zeros\n",
    "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
    "\n",
    "    # Loop through each position in the context window\n",
    "    for position in range(context_window):\n",
    "        # Loop through each dimension in the embedding\n",
    "        for i in range(embedding_dim // 2):\n",
    "            # Calculate the rotation angle (theta) based on the position and embedding dimension\n",
    "            theta = 10000. ** (-2. * (i - 1) / embedding_dim)\n",
    "            # Calculate the rotated matrix elements using sine and cosine functions\n",
    "            m_theta = position * theta\n",
    "            R[position, 2 * i, 2 * i] = np.cos(m_theta)\n",
    "            R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)\n",
    "            R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)\n",
    "            R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bSGLl26wBzZS",
   "metadata": {
    "id": "bSGLl26wBzZS"
   },
   "outputs": [],
   "source": [
    "class RoPEAttentionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Linear transformation for query\n",
    "        self.w_q = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
    "        # Linear transformation for key\n",
    "        self.w_k = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
    "        # Linear transformation for value\n",
    "        self.w_v = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
    "        # Obtain rotary matrix for positional embeddings\n",
    "        self.R = get_rotary_matrix(config['context_window'], config['d_model'])\n",
    "\n",
    "    def get_rotary_matrix(context_window, embedding_dim):\n",
    "        # Generate rotational matrix for RoPE\n",
    "        R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
    "        for position in range(context_window):\n",
    "            for i in range(embedding_dim//2):\n",
    "\n",
    "                # Rest of the code\n",
    "                ...\n",
    "\n",
    "        return R\n",
    "\n",
    "    def forward(self, x, return_attn_weights=False):\n",
    "        # x: input tensor of shape (batch, sequence length, dimension)\n",
    "\n",
    "        b, m, d = x.shape  # batch size, sequence length, dimension\n",
    "\n",
    "        # Linear transformations for Q, K, and V\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        # Rotate Q and K using the RoPE matrix\n",
    "        q_rotated = (torch.bmm(q.transpose(0, 1), self.R[:m])).transpose(0, 1)\n",
    "        k_rotated = (torch.bmm(k.transpose(0, 1), self.R[:m])).transpose(0, 1)\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        activations = F.scaled_dot_product_attention(\n",
    "            q_rotated, k_rotated, v, dropout_p=0.1, is_causal=True\n",
    "        )\n",
    "\n",
    "        if return_attn_weights:\n",
    "            # Create a causal attention mask\n",
    "            attn_mask = torch.tril(torch.ones((m, m)), diagonal=0)\n",
    "            # Calculate attention weights and add causal mask\n",
    "            attn_weights = torch.bmm(q_rotated, k_rotated.transpose(1, 2)) / np.sqrt(d) + attn_mask\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "            return activations, attn_weights\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FubdHHNTBzec",
   "metadata": {
    "id": "FubdHHNTBzec"
   },
   "outputs": [],
   "source": [
    "class RoPEMaskedMultiheadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Create a list of RoPEMaskedAttentionHead instances as attention heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            RoPEMaskedAttentionHead(config) for _ in range(config['n_heads'])\n",
    "        ])\n",
    "        self.linear = nn.Linear(config['n_heads'] * config['d_model'], config['d_model'])  # Linear layer after concatenating heads\n",
    "        self.dropout = nn.Dropout(.1)  # Dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input tensor of shape (batch, sequence length, dimension)\n",
    "\n",
    "        # Process each attention head and concatenate the results\n",
    "        heads = [h(x) for h in self.heads]\n",
    "        x = torch.cat(heads, dim=-1)\n",
    "\n",
    "        # Apply linear transformation to the concatenated output\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wZ1hhcLHBzkw",
   "metadata": {
    "id": "wZ1hhcLHBzkw"
   },
   "outputs": [],
   "source": [
    "# Update the master configuration with the number of attention heads\n",
    "MASTER_CONFIG.update({\n",
    "    'n_heads': 8,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bNyCiJA8BzpR",
   "metadata": {
    "id": "bNyCiJA8BzpR"
   },
   "outputs": [],
   "source": [
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding layer for input tokens\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "\n",
    "        # RMSNorm layer for pre-normalization\n",
    "        self.rms = RMSNorm((config['context_window'], config['d_model']))\n",
    "\n",
    "        # RoPEMaskedMultiheadAttention layer\n",
    "        self.rope_attention = RoPEMaskedMultiheadAttention(config)\n",
    "\n",
    "        # Linear layer followed by ReLU activation\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Final linear layer for prediction\n",
    "        self.last_linear = nn.Linear(config['d_model'], config['vocab_size'])\n",
    "\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx: input indices\n",
    "        x = self.embedding(idx)\n",
    "\n",
    "        # One block of attention\n",
    "        x = self.rms(x)  # RMS pre-normalization\n",
    "        x = x + self.rope_attention(x)\n",
    "\n",
    "        x = self.rms(x)  # RMS pre-normalization\n",
    "        x = x + self.linear(x)\n",
    "\n",
    "        logits = self.last_linear(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O8ki0dOoBgtu",
   "metadata": {
    "id": "O8ki0dOoBgtu"
   },
   "outputs": [],
   "source": [
    "# Create an instance of RopeModel (RMSNorm, RoPE, Multi-Head)\n",
    "model = RopeModel(MASTER_CONFIG)\n",
    "\n",
    "# Obtain batches for training\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Calculate logits and loss using the model\n",
    "logits, loss = model(xs, ys)\n",
    "\n",
    "# Define the Adam optimizer for model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MXOSgVBYCO7y",
   "metadata": {
    "id": "MXOSgVBYCO7y"
   },
   "outputs": [],
   "source": [
    "# Updating training configuration with more epochs and a logging interval\n",
    "MASTER_CONFIG.update({\n",
    "    \"epochs\": 5000,\n",
    "    \"log_interval\": 10,\n",
    "})\n",
    "\n",
    "# Training the model with the updated configuration\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-odXc03hCPIv",
   "metadata": {
    "id": "-odXc03hCPIv"
   },
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    \"\"\" Paper Link -> https://arxiv.org/pdf/2002.05202v1.pdf \"\"\"\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.config = config  # Configuration information\n",
    "        self.linear_gate = nn.Linear(size, size)  # Linear transformation for the gating mechanism\n",
    "        self.linear = nn.Linear(size, size)  # Linear transformation for the main branch\n",
    "        self.beta = torch.randn(1, requires_grad=True)  # Random initialization of the beta parameter\n",
    "\n",
    "        # Using nn.Parameter for beta to ensure it's recognized as a learnable parameter\n",
    "        self.beta = nn.Parameter(torch.ones(1))\n",
    "        self.register_parameter(\"beta\", self.beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Swish-Gated Linear Unit computation\n",
    "        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))\n",
    "        out = swish_gate * self.linear(x)  # Element-wise multiplication of the gate and main branch\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DM1Pe9BgCcPx",
   "metadata": {
    "id": "DM1Pe9BgCcPx"
   },
   "outputs": [],
   "source": [
    "class RopeModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding layer for input tokens\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "\n",
    "        # RMSNorm layer for pre-normalization\n",
    "        self.rms = RMSNorm((config['context_window'], config['d_model']))\n",
    "\n",
    "        # Multi-head attention layer with RoPE (Rotary Positional Embeddings)\n",
    "        self.rope_attention = RoPEMaskedMultiheadAttention(config)\n",
    "\n",
    "        # Linear layer followed by SwiGLU activation\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            SwiGLU(config['d_model']),  # Adding SwiGLU activation\n",
    "        )\n",
    "\n",
    "        # Output linear layer\n",
    "        self.last_linear = nn.Linear(config['d_model'], config['vocab_size'])\n",
    "\n",
    "        # Printing total model parameters\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embedding(idx)\n",
    "\n",
    "        # One block of attention\n",
    "        x = self.rms(x)  # RMS pre-normalization\n",
    "        x = x + self.rope_attention(x)\n",
    "\n",
    "        x = self.rms(x)  # RMS pre-normalization\n",
    "        x = x + self.linear(x)  # Applying SwiGLU activation\n",
    "\n",
    "        logits = self.last_linear(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # Calculate cross-entropy loss if targets are provided\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wOjeTFIICcbo",
   "metadata": {
    "id": "wOjeTFIICcbo"
   },
   "outputs": [],
   "source": [
    "# Create an instance of RopeModel (RMSNorm, RoPE, Multi-Head, SwiGLU)\n",
    "model = RopeModel(MASTER_CONFIG)\n",
    "\n",
    "# Obtain batches for training\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Calculate logits and loss using the model\n",
    "logits, loss = model(xs, ys)\n",
    "\n",
    "# Define the Adam optimizer for model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fwPFZ-g6Cp4R",
   "metadata": {
    "id": "fwPFZ-g6Cp4R"
   },
   "outputs": [],
   "source": [
    "# Update model configurations for the number of layers\n",
    "MASTER_CONFIG.update({\n",
    "    'n_layers': 4,  # Set the number of layers to 4\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YgSyRLmICqFh",
   "metadata": {
    "id": "YgSyRLmICqFh"
   },
   "outputs": [],
   "source": [
    "class LlamaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # RMSNorm layer\n",
    "        self.rms = RMSNorm((config['context_window'], config['d_model']))\n",
    "\n",
    "        # RoPE Masked Multihead Attention layer\n",
    "        self.attention = RoPEMaskedMultiheadAttention(config)\n",
    "\n",
    "        # Feedforward layer with SwiGLU activation\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            SwiGLU(config['d_model']),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # one block of attention\n",
    "        x = self.rms(x) # RMS pre-normalization\n",
    "        x = x + self.attention(x)  # residual connection\n",
    "\n",
    "        x = self.rms(x) # RMS pre-normalization\n",
    "        x = x + self.feedforward(x)  # residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mK72zqr-CxPI",
   "metadata": {
    "id": "mK72zqr-CxPI"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the LlamaBlock class with the provided configuration\n",
    "block = LlamaBlock(MASTER_CONFIG)\n",
    "\n",
    "# Generate a random tensor with the specified batch size, context window, and model dimension\n",
    "random_input = torch.randn(MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'], MASTER_CONFIG['d_model'])\n",
    "\n",
    "# Apply the LlamaBlock to the random input tensor\n",
    "output = block(random_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sBn_84ktCxkh",
   "metadata": {
    "id": "sBn_84ktCxkh"
   },
   "outputs": [],
   "source": [
    "class Llama(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Embedding layer for token representations\n",
    "        self.embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        # Sequential block of LlamaBlocks based on the specified number of layers\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"llama_{i}\", LlamaBlock(config)) for i in range(config['n_layers'])])\n",
    "        )\n",
    "        # Feedforward network (FFN) for final output\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            SwiGLU(config['d_model']),\n",
    "            nn.Linear(config['d_model'], config['vocab_size']),\n",
    "        )\n",
    "\n",
    "        # Print total number of parameters in the model\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Input token indices are passed through the embedding layer\n",
    "        x = self.embeddings(idx)\n",
    "        # Process the input through the LlamaBlocks\n",
    "        x = self.llama_blocks(x)\n",
    "        # Pass the processed input through the final FFN for output logits\n",
    "        logits = self.ffn(x)\n",
    "\n",
    "        # If targets are not provided, return only the logits\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        # If targets are provided, compute and return the cross-entropy loss\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E1M5yLUKCxun",
   "metadata": {
    "id": "E1M5yLUKCxun"
   },
   "outputs": [],
   "source": [
    "# Create an instance of RopeModel (RMSNorm, RoPE, Multi-Head, SwiGLU, N_layers)\n",
    "llama = Llama(MASTER_CONFIG)\n",
    "\n",
    "# Obtain batches for training\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Calculate logits and loss using the model\n",
    "logits, loss = llama(xs, ys)\n",
    "\n",
    "# Define the Adam optimizer for model parameters\n",
    "optimizer = torch.optim.Adam(llama.parameters())\n",
    "\n",
    "# Train the model\n",
    "train(llama, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82vHod_RDkAT",
   "metadata": {
    "id": "82vHod_RDkAT"
   },
   "outputs": [],
   "source": [
    "# Update the number of epochs in the configuration\n",
    "MASTER_CONFIG.update({\n",
    "    'epochs': 10000,\n",
    "})\n",
    "# Train the LLaMA model for the specified number of epochs\n",
    "train(llama, optimizer, scheduler=None, config=MASTER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fAvGl-uhDkIr",
   "metadata": {
    "id": "fAvGl-uhDkIr"
   },
   "outputs": [],
   "source": [
    "# Training the model again, scheduler for better optimization.\n",
    "train(llama, optimizer, config=MASTER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A6F4cK5ZDkQE",
   "metadata": {
    "id": "A6F4cK5ZDkQE"
   },
   "outputs": [],
   "source": [
    "# Generate text using the trained LLM (llama) with a maximum of 500 tokens\n",
    "generated_text = generate(llama, MASTER_CONFIG, 500)[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Okm_7_WJEBvm",
   "metadata": {
    "id": "Okm_7_WJEBvm"
   },
   "outputs": [],
   "source": [
    "# Get batches from the test set\n",
    "xs, ys = get_batches(dataset, 'test', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Pass the test data through the LLaMA model\n",
    "logits, loss = llama(xs, ys)\n",
    "\n",
    "# Print the loss on the test set\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E-FC9fnzDkUj",
   "metadata": {
    "id": "E-FC9fnzDkUj"
   },
   "outputs": [],
   "source": [
    "# Update configuration\n",
    "MASTER_CONFIG.update({\n",
    "    \"epochs\": 1000\n",
    "})\n",
    "\n",
    "# Create Llama model with Cosine Annealing learning schedule\n",
    "llama_with_cosine = Llama(MASTER_CONFIG)\n",
    "\n",
    "# Define Adam optimizer with specific hyperparameters\n",
    "llama_optimizer = torch.optim.Adam(\n",
    "    llama.parameters(),\n",
    "    betas=(.9, .95),\n",
    "    weight_decay=.1,\n",
    "    eps=1e-9,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# Define Cosine Annealing learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(llama_optimizer, 300, eta_min=1e-5)\n",
    "\n",
    "# Train the Llama model with the specified optimizer and scheduler\n",
    "train(llama_with_cosine, llama_optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WTy4K3u6ENty",
   "metadata": {
    "id": "WTy4K3u6ENty"
   },
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "torch.save(llama, 'llama_model.pth')\n",
    "\n",
    "# If you want to save only the model parameters\n",
    "torch.save(llama.state_dict(), 'llama_model_params.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HslEoi0qESVw",
   "metadata": {
    "id": "HslEoi0qESVw"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# Assuming Llama is your PyTorch model\n",
    "llama_config = GPT2Config.from_dict(MASTER_CONFIG)\n",
    "llama_transformers = GPT2LMHeadModel(config=llama_config)\n",
    "llama_transformers.load_state_dict(llama.state_dict())\n",
    "\n",
    "# Specify the directory where you want to save the model\n",
    "output_dir = \"llama_model_transformers\"\n",
    "\n",
    "# Save the model and configuration\n",
    "llama_transformers.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C0EGZ3tiESfT",
   "metadata": {
    "id": "C0EGZ3tiESfT"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# Specify the directory where the model was saved\n",
    "output_dir = \"llama_model_transformers\"\n",
    "\n",
    "# Load the model and configuration\n",
    "llama_transformers = GPT2LMHeadModel.from_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
